# ADR-047: PolÃ­tica de Confianza para Sub-Agentes

> **Status:** âš ï¸ SUPERADA por ADR-048 (Pool-Based Scoring) â€” Pendiente validaciÃ³n PO
> **Date:** 2026-02-23
> **Author:** Arquitecto AICE (sub-agent)
> **Deciders:** Sergio (PO), ComPi (Orquestador)
> **Superseded by:** ADR-048 â€” El PO decidiÃ³ pool-based scoring sin fases en lugar del Modelo D con fases

---

## 1. Contexto

AICE fue diseÃ±ado inicialmente para un agente Ãºnico (ComPi) evaluado por un humano (Sergio). Sin embargo, ComPi opera con un **equipo de sub-agentes** con la siguiente jerarquÃ­a:

```
Sergio (humano, PO)
  â””â”€â”€ ComPi (agente principal, Opus 4.6 + thinking high)
        â”œâ”€â”€ Arquitecto (Opus 4.6 + thinking high)
        â”‚     â”œâ”€â”€ Backend Developer (Sonnet 4.5 + thinking high)
        â”‚     â”œâ”€â”€ Frontend Developer (Sonnet 4.5 + thinking high)
        â”‚     â”œâ”€â”€ Data Engineer (Sonnet 4.5 + thinking high)
        â”‚     â””â”€â”€ Tester/QA (Sonnet 4.5 + thinking high)
        â””â”€â”€ [Otros arquitectos para otros proyectos]
```

**Flujo de evaluaciÃ³n actual:**
- Sergio â†” ComPi (interacciÃ³n directa, evalÃºa explÃ­cita e implÃ­citamente)
- ComPi â†’ Arquitectos (revisa outputs, evalÃºa implÃ­citamente)
- Arquitectos â†’ Equipo (revisan entregables, evalÃºan implÃ­citamente)
- Sergio NO interactÃºa con sub-agentes directamente

**Pregunta abierta:** Â¿CÃ³mo debe AICE modelar la confianza cuando hay mÃºltiples agentes con diferentes modelos, niveles de responsabilidad y evaluadores?

---

## 2. Preguntas del PO

1. Â¿Los sub-agentes deberÃ­an tener su propio AICE score independiente?
2. Â¿Es parte de la confianza anidada que el usuario tiene en el agente principal?
3. Si un sub-agente falla, Â¿se penaliza al agente principal? Â¿Siempre? Â¿Depende de la causa?
4. Si usan modelos distintos (ej: Opus vs Sonnet), Â¿deben tener puntuaciones separadas? Â¿Son comparables?
5. Â¿CÃ³mo afecta esto a la velocidad de maduraciÃ³n?

---

## 3. Modelos Evaluados

### 3.1 Modelo A â€” Solo el Agente Principal Tiene Score

**Concepto:** Sub-agentes son "herramientas" de ComPi. Solo ComPi tiene `confidence.json`. Si un sub-agente falla, ComPi paga en ORCH (o en el dominio afectado).

**MecÃ¡nica:**
```
on_subagent_failure(subagent, error):
  compi.domains.ORCH.score -= penalty(error.severity)
  # El sub-agente no tiene score propio

on_subagent_success(subagent):
  # Contribuye a racha de ComPi en ORCH
  compi.domains.ORCH.streak += 1
```

**Pros:**
- âœ… MÃ¡xima simplicidad â€” zero cambios en confidence.json schema
- âœ… Coherente con la visiÃ³n del usuario: "yo evalÃºo a ComPi, Ã©l responde por todo"
- âœ… No introduce comparabilidad entre modelos (no hay scores de Sonnet vs Opus)
- âœ… Implementable AHORA sin cambios en skill ni servidor central
- âœ… ComPi tiene incentivo real de supervisar bien a su equipo (su ORCH depende de ello)

**Contras:**
- âŒ **Sin granularidad:** Si el Backend falla repetidamente pero el Frontend es excelente, ComPi no puede diagnosticar dÃ³nde estÃ¡ el problema
- âŒ **ORCH sobrecargado:** El dominio ORCH absorbe toda la varianza del equipo â€” se vuelve ruidoso
- âŒ **Injusto para ComPi:** Un error genuino de Sonnet (que ComPi no podÃ­a prevenir con mejor contexto) penaliza su score
- âŒ **Sin seÃ±al para el usuario:** Sergio no sabe si puede confiar en el equipo o solo en ComPi
- âŒ **Sin datos para mejorar:** No hay historial de quÃ© sub-agente falla mÃ¡s ni por quÃ©

---

### 3.2 Modelo B â€” Cada Agente Tiene Score Independiente

**Concepto:** Cada sub-agente tiene su propio `confidence.json`. Scores completamente independientes. Cadena de evaluaciÃ³n: Sergioâ†’ComPi, ComPiâ†’Arquitectos, Arquitectosâ†’Equipo.

**MecÃ¡nica:**
```
# Cada agente tiene su confidence.json independiente
subagent_confidence = {
  "agentId": "aice-backend-dev",
  "runtime": { "model": "anthropic/claude-sonnet-4-5", "thinking": "high" },
  "domains": { ... },  # 5 dominios completos
  "evaluatedBy": "aice-architect",  # QuiÃ©n lo evalÃºa
  ...
}
```

**Pros:**
- âœ… MÃ¡xima granularidad â€” cada agente tiene su historial completo
- âœ… DiagnÃ³stico preciso: "el Backend tiene TECH bajo, el Frontend va bien"
- âœ… Datos ricos para el servicio central (multi-agente desde dÃ­a 1)
- âœ… Cada agente madura a su ritmo (maturity independiente)

**Contras:**
- âŒ **Â¿QuiÃ©n evalÃºa a quiÃ©n?** Cadena de evaluaciÃ³n problemÃ¡tica:
  - Sergio evalÃºa a ComPi â† humano, fiable âœ…
  - ComPi (Opus) evalÃºa a Arquitecto (Opus) â† comparable, aceptable âš ï¸
  - Arquitecto (Opus) evalÃºa a Backend (Sonnet) â† Â¿es justo? El evaluador es mÃ¡s capaz que el evaluado âš ï¸
  - Backend (Sonnet) evalÃºa... Â¿a nadie? Cadena truncada â“
- âŒ **Comparabilidad entre modelos:** Un Sonnet al 80% NO es igual que un Opus al 80%. Las expectativas base son distintas.
- âŒ **InflaciÃ³n de scores:** Sin evaluaciÃ³n humana directa, scores de sub-agentes pueden inflarse (auto-complacencia entre agentes)
- âŒ **Complejidad explosiva:** Con 6 agentes Ã— 5 dominios Ã— rachas Ã— maturity = 30 scores a mantener
- âŒ **Anti-gaming difÃ­cil:** Un agente podrÃ­a evaluar benÃ©volamente a otro para mejorar el rendimiento percibido del equipo
- âŒ **Sub-agentes efÃ­meros:** Los sub-agentes se crean y destruyen por proyecto. Â¿QuÃ© pasa con sus scores cuando el proyecto termina?
- âŒ **Overhead de evaluaciÃ³n:** Genera muchos archivos, logs, complejidad operativa

---

### 3.3 Modelo C â€” Confianza en Cascada

**Concepto:** ComPi tiene el score visible para Sergio. Sub-agentes tienen scores internos que ComPi mantiene en un registro centralizado. El ORCH de ComPi depende parcialmente del rendimiento del equipo. Los fallos se trazan para atribuir responsabilidad.

**MecÃ¡nica:**
```
on_subagent_failure(subagent, error):
  # 1. Trazabilidad: Â¿de quiÃ©n es la culpa?
  cause = trace_root_cause(error)
  
  if cause == "BAD_CONTEXT":
    # ComPi dio mal contexto al sub-agente
    compi.domains.ORCH.score -= penalty(error.severity)
    subagent.internal_score -= penalty(error.severity) * 0.5
  elif cause == "SUBAGENT_ERROR":
    # Error genuino del sub-agente
    subagent.internal_score -= penalty(error.severity)
    compi.domains.ORCH.score -= penalty(error.severity) * 0.25  # penalizaciÃ³n reducida por supervisiÃ³n
  elif cause == "SPEC_AMBIGUITY":
    # La spec era ambigua (responsabilidad del Arquitecto)
    architect.internal_score -= penalty(error.severity)
    compi.domains.ORCH.score -= 0  # ComPi no es responsable de specs del arquitecto

on_team_success():
  # Todos suben
  compi.domains.ORCH.streak += 1
  for agent in team:
    agent.internal_score += delta_for_role(agent.role)
```

**Pros:**
- âœ… AtribuciÃ³n justa de responsabilidad (no todo cae en ComPi)
- âœ… ComPi mantiene visibilidad interna de su equipo
- âœ… Sergio ve un solo score (ComPi) pero puede pedir detalle del equipo
- âœ… Incentivos alineados: ComPi quiere que su equipo mejore (su ORCH depende de ello)
- âœ… Trazabilidad de fallos permite diagnÃ³stico y mejora

**Contras:**
- âŒ **Complejidad de atribuciÃ³n:** Decidir si la causa es "bad context" vs "subagent error" vs "spec ambiguity" es subjetivo y difÃ­cil de automatizar
- âŒ **Scores internos â‰  scores reales:** Sin evaluaciÃ³n humana, los scores internos son estimaciones del agente sobre sÃ­ mismo
- âŒ **QuiÃ©n traza la culpa?** ComPi es juez y parte â€” puede atribuir culpa al sub-agente para proteger su propio ORCH
- âŒ **Complejidad media-alta:** MÃ¡s que A, menos que B, pero requiere lÃ³gica de atribuciÃ³n

---

### 3.4 Modelo D (PROPUESTA) â€” Cascada Simplificada con Registro de Equipo

**Concepto:** EvoluciÃ³n de C que resuelve los problemas de atribuciÃ³n y anti-gaming. ComPi tiene el score pÃºblico (evaluado por Sergio). Sub-agentes tienen un **registro de rendimiento** (no un AICE score completo) que ComPi mantiene como parte de su propio `confidence.json`. La atribuciÃ³n de culpa sigue reglas determinÃ­sticas, no discrecionales.

**Principios:**
1. **Un solo score pÃºblico** (ComPi) â€” el que Sergio ve y evalÃºa
2. **Registro de rendimiento del equipo** â€” datos internos, no scores AICE completos
3. **Reglas determinÃ­sticas de atribuciÃ³n** â€” sin discrecionalidad del agente
4. **Transparencia bajo demanda** â€” Sergio puede pedir "Â¿cÃ³mo va el equipo?"
5. **ProgresiÃ³n natural** â€” de D (MVP) a B (futuro con servidor central)

**MecÃ¡nica detallada en Â§6 (Schema propuesto).**

---

## 4. Tabla Comparativa

| Criterio | A (Solo Principal) | B (Independiente) | C (Cascada) | D (Cascada Simplificada) |
|----------|:--:|:--:|:--:|:--:|
| **Complejidad de implementaciÃ³n** | ðŸŸ¢ MÃ­nima | ðŸ”´ Alta | ðŸŸ  Media-alta | ðŸŸ¡ Media-baja |
| **Granularidad diagnÃ³stica** | ðŸ”´ Nula | ðŸŸ¢ MÃ¡xima | ðŸŸ¢ Alta | ðŸŸ¡ Buena |
| **Justicia para el principal** | ðŸ”´ Injusto | ðŸŸ¢ Justo | ðŸŸ¡ Depende de atribuciÃ³n | ðŸŸ¢ Justo |
| **Resistencia a gaming** | ðŸŸ¢ Alta (humano evalÃºa) | ðŸ”´ Baja (agentes evalÃºan agentes) | ðŸŸ  Media (juez y parte) | ðŸŸ¢ Alta (reglas determinÃ­sticas) |
| **Transparencia para usuario** | ðŸ”´ Opaca | ðŸŸ¢ Transparente | ðŸŸ¡ Bajo demanda | ðŸŸ¢ Bajo demanda |
| **Comparabilidad entre modelos** | N/A | ðŸ”´ Problema | ðŸŸ  Problema parcial | ðŸŸ¢ Normalizado |
| **Velocidad de maduraciÃ³n** | ðŸ”´ Lenta (solo 1 agente) | ðŸŸ¢ RÃ¡pida | ðŸŸ¡ Media | ðŸŸ¡ Media |
| **Complejidad de schema** | ðŸŸ¢ Sin cambios | ðŸ”´ NÃ—confidence.json | ðŸŸ  Schema complejo | ðŸŸ¡ ExtensiÃ³n limpia |
| **Implementable en MVP** | ðŸŸ¢ Ahora | ðŸ”´ Requiere API | ðŸŸ  Parcial | ðŸŸ¢ Ahora |
| **Escalabilidad futura** | ðŸ”´ Dead end | ðŸŸ¢ Full | ðŸŸ¢ Full | ðŸŸ¢ Evoluciona a B |
| **Sub-agentes efÃ­meros** | N/A | ðŸ”´ Problema (Â¿quÃ© pasa con scores?) | ðŸŸ¡ Registros se archivan | ðŸŸ¢ Natural (registro ligero) |

---

## 5. RecomendaciÃ³n: Modelo D en Fases

### Fase 1 (MVP â€” Ahora, sin API central)

**Implementar Modelo D â€” Cascada Simplificada.**

**JustificaciÃ³n:**
1. **Implementable hoy** â€” ExtensiÃ³n de `confidence.json`, no requiere servidor central
2. **Resuelve la pregunta del PO** â€” Sergio puede preguntar "Â¿cÃ³mo va el equipo?" y obtener respuesta informada
3. **No rompe nada** â€” El score pÃºblico de ComPi sigue siendo el que Sergio evalÃºa
4. **Datos valiosos** â€” Cuando llegue el API (Fase 2 del roadmap), ya hay datos de equipo acumulados
5. **Anti-gaming incorporado** â€” Reglas determinÃ­sticas, no discrecionales

**QuÃ© se implementa:**
- Campo `teamRegistry` en `confidence.json` de ComPi
- Reglas de atribuciÃ³n de culpa (Â§5.1)
- Impacto en ORCH definido por fÃ³rmulas, no por juicio del agente
- Comando `/aice team` para que Sergio vea rendimiento del equipo

### Fase 2 (Con API central â€” Futuro)

**Evolucionar a Modelo B parcial** para sub-agentes que lo ameriten (persistentes, cross-proyecto).

**QuÃ© se aÃ±ade:**
- Arquitectos persistentes (que trabajan en mÃºltiples proyectos) obtienen score AICE propio
- Score normalizado por modelo (ver Â§5.3)
- EvaluaciÃ³n cruzada con validaciÃ³n del servidor central (anti-gaming)
- Sub-agentes efÃ­meros siguen en registro del equipo (no merecen score completo)

### Fase 3 (Multi-agente maduro)

**Modelo B completo** con:
- Leaderboard multi-agente (scores normalizados)
- EvaluaciÃ³n inter-agente validada
- Perfiles de rendimiento por modelo
- MÃ©tricas de equipo agregadas

---

## 5.1 Reglas de AtribuciÃ³n de Culpa (DeterminÃ­sticas)

**Principio:** Las reglas deben ser aplicables sin juicio discrecional del agente. Son binary checks, no evaluaciones subjetivas.

### Regla 1: DELEGATION_QUALITY (Â¿ComPi dio buen contexto?)

```
CHECK: Â¿El sub-agente recibiÃ³ toda la informaciÃ³n necesaria para la tarea?
  - Â¿Se especificÃ³ el objetivo claramente?
  - Â¿Se proporcionaron los archivos de referencia?
  - Â¿Se definieron criterios de aceptaciÃ³n?

SI faltan â‰¥ 2 de estos â†’ DELEGATION_QUALITY_FAIL
  â†’ ComPi.ORCH: penalizaciÃ³n completa
  â†’ Sub-agente: 0 (no es su culpa)

SI todo presente â†’ continuar a Regla 2
```

### Regla 2: EXECUTION_QUALITY (Â¿El sub-agente ejecutÃ³ bien?)

```
CHECK: Con el contexto dado, Â¿el output cumple los criterios?
  - Â¿El resultado es correcto tÃ©cnicamente?
  - Â¿Se siguieron las instrucciones?
  - Â¿Hay errores propios del sub-agente?

SI output incorrecto CON buen contexto â†’ EXECUTION_FAIL
  â†’ Sub-agente: penalizaciÃ³n en teamRegistry
  â†’ ComPi.ORCH: penalizaciÃ³n reducida (Ã—0.25) â€” debiÃ³ revisar antes de aceptar

SI output correcto â†’ SUCCESS
  â†’ Sub-agente: registro positivo
  â†’ ComPi.ORCH: contribuye a streak
```

### Regla 3: REVIEW_QUALITY (Â¿ComPi revisÃ³ el output?)

```
CHECK: Â¿ComPi entregÃ³ el output del sub-agente a Sergio sin revisarlo?

SI ComPi pasÃ³ output sin revisar Y Sergio encontrÃ³ error â†’ REVIEW_FAIL
  â†’ ComPi.ORCH: penalizaciÃ³n completa (debiÃ³ revisar)
  â†’ Sub-agente: penalizaciÃ³n en registro (produjo output defectuoso)

SI ComPi revisÃ³ y corrigiÃ³ antes de entregar â†’ REVIEW_CATCH
  â†’ ComPi.ORCH: pro-patrÃ³n (buena supervisiÃ³n)
  â†’ Sub-agente: registro de error (pero capturado)
```

### Tabla resumen de atribuciÃ³n

| Escenario | ComPi ORCH | Sub-agente registro | Ejemplo |
|-----------|:---:|:---:|---------|
| Mal contexto â†’ sub-agente falla | **âˆ’100%** | 0% | ComPi no pasÃ³ la spec al backend |
| Buen contexto â†’ sub-agente falla | **âˆ’25%** | âˆ’100% | Backend hizo bug con spec clara |
| ComPi no revisÃ³ â†’ Sergio encontrÃ³ error | **âˆ’100%** | âˆ’100% | ComPi pasÃ³ cÃ³digo roto sin revisar |
| ComPi revisÃ³ y corrigiÃ³ | **Pro-patrÃ³n** | error (capturado) | ComPi detectÃ³ el bug antes de entregar |
| Todo bien â†’ output correcto | **+streak** | +registro positivo | Flujo limpio |

**Notas sobre los porcentajes:**
- "âˆ’100%" = penalizaciÃ³n completa segÃºn severidad del error (ðŸŸ¡-1, ðŸŸ -3, ðŸ”´-5, âš«-10)
- "âˆ’25%" = 25% de la penalizaciÃ³n base (ej: error grave -5 â†’ ComPi recibe -1.25 â†’ -1 redondeado)
- El redondeo es siempre hacia el mÃ¡s punitivo (ceiling de penalizaciÃ³n)

---

## 5.2 Respuestas a las Preguntas del PO

### P1: Â¿Los sub-agentes deberÃ­an tener su propio AICE score independiente?

**Fase 1 (MVP): NO.** Tienen un **registro de rendimiento** dentro del `confidence.json` de ComPi. No es un score AICE completo (no tiene 5 dominios, rachas, maturity, etc.). Es un tracking simplificado: tareas asignadas, tareas completadas, errores, Ã©xitos, y un Ã­ndice de fiabilidad.

**Fase 2+: SÃ, para agentes persistentes** (arquitectos cross-proyecto). Score AICE completo, normalizado por modelo.

**JustificaciÃ³n:** Un sub-agente efÃ­mero (backend de un proyecto) no justifica un score completo con maturity y warmup. Un arquitecto que trabaja en 5 proyectos sÃ­.

### P2: Â¿Es parte de la confianza anidada que el usuario tiene en el agente principal?

**SÃ, parcialmente.** El ORCH de ComPi refleja su capacidad de orquestar (elegir bien, dar buen contexto, revisar, corregir). Un mal equipo baja el ORCH de ComPi, pero las reglas de atribuciÃ³n protegen a ComPi de errores genuinos del sub-agente (solo -25%).

**Transparencia:** Sergio puede preguntar "Â¿cÃ³mo va el equipo?" y ComPi presenta el `teamRegistry` con datos factuales. Sergio evalÃºa a ComPi sabiendo cÃ³mo gestiona su equipo.

### P3: Si un sub-agente falla, Â¿se penaliza al agente principal?

**Depende de la causa (reglas determinÃ­sticas Â§5.1):**

| Causa | ComPi penalizado? |
|-------|:-:|
| ComPi dio mal contexto | **SÃ, 100%** |
| Sub-agente fallÃ³ con buen contexto | **SÃ, 25%** (debiÃ³ supervisar) |
| ComPi no revisÃ³ antes de entregar | **SÃ, 100%** |
| ComPi revisÃ³ y capturÃ³ el error | **NO, pro-patrÃ³n** |

**El 25% residual** se justifica porque ComPi ELIGIÃ“ delegar a ese sub-agente y DECIDIÃ“ no supervisar mÃ¡s de cerca. La capacidad de supervisiÃ³n es parte del ORCH.

### P4: Si usan modelos distintos, Â¿deben tener puntuaciones separadas? Â¿Son comparables?

**Fase 1: No son comparables directamente. Por eso el registro de equipo NO es un score AICE.** Es un registro factual (tareas/errores/Ã©xitos) sin la pretensiÃ³n de ser comparable con el score de ComPi.

**Fase 2+: NormalizaciÃ³n por modelo.** El servidor central acumula datos de rendimiento por modelo. Un "80% Sonnet" y un "80% Opus" significan cosas distintas. La normalizaciÃ³n:

```
normalized_score = raw_score * model_baseline_factor

Ejemplo (hipotÃ©tico, con datos reales del servidor):
  opus_baseline = 1.0 (referencia)
  sonnet_baseline = 0.85 (rinde ~85% respecto a Opus en evaluaciones reales)
  
  Sonnet raw 80% â†’ normalized: 80% * (1/0.85) = 94% (ajustado por expectativa menor)
  Opus raw 80% â†’ normalized: 80% * (1/1.0) = 80%
```

**Importante:** Los baselines por modelo se calculan con datos REALES del servidor central, no se inventan. Hasta tener esos datos (Fase 2), no se normalizan â€” se reportan con la etiqueta del modelo.

### P5: Â¿CÃ³mo afecta esto a la velocidad de maduraciÃ³n?

**CÃ¡lculos detallados en Â§7.**

---

## 5.3 Comparabilidad entre Modelos â€” DiseÃ±o de NormalizaciÃ³n

### Problema
Un Sonnet al 80% y un Opus al 80% no significan lo mismo. Sonnet tiene capacidades base mÃ¡s limitadas. Un Opus cometiendo el mismo error que un Sonnet es mÃ¡s grave (se esperaba mÃ¡s de Ã©l).

### SoluciÃ³n propuesta (Fase 2)

**Model Performance Profile (MPP):**

```json
{
  "modelProfiles": {
    "anthropic/claude-opus-4-6": {
      "baselineMultiplier": 1.0,
      "expectedErrorRate": 0.05,
      "sampleSize": 2500,
      "lastUpdated": "2026-06-15"
    },
    "anthropic/claude-sonnet-4-5": {
      "baselineMultiplier": 0.85,
      "expectedErrorRate": 0.12,
      "sampleSize": 8000,
      "lastUpdated": "2026-06-15"
    }
  }
}
```

**AplicaciÃ³n:**
- Scores RAW se muestran siempre (sin manipular)
- Scores NORMALIZED se muestran como tooltip/detalle
- El leaderboard permite filtrar por modelo o ver normalizado
- Los baselines se calculan del dataset real del servidor central

**Fase 1 (MVP):** No se normaliza. Se muestra el modelo como metadata. El registro del equipo indica quÃ© modelo usa cada sub-agente.

---

## 6. Schema Propuesto (Modelo D)

### 6.1 ExtensiÃ³n de `confidence.json`

Se aÃ±ade el campo `teamRegistry` al `confidence.json` existente de ComPi:

```json
{
  "version": 8,
  "agentId": "compi",
  "runtime": { ... },
  "domains": { ... },
  
  "teamRegistry": {
    "version": 1,
    "description": "Registro de rendimiento de sub-agentes gestionados por este agente",
    "agents": {
      "aice-architect": {
        "role": "architect",
        "model": "anthropic/claude-opus-4-6",
        "thinking": "high",
        "project": "ai-confidence",
        "persistent": true,
        "stats": {
          "tasksAssigned": 12,
          "tasksCompleted": 11,
          "tasksWithErrors": 2,
          "errorsCapturated": 1,
          "reliability": 0.83,
          "lastActivity": "2026-02-23T12:30:00Z"
        },
        "recentEvals": [
          {
            "timestamp": "2026-02-23T12:30:00Z",
            "task": "ADR-047 trust policy analysis",
            "result": "success",
            "quality": "high",
            "note": "AnÃ¡lisis completo y bien fundamentado"
          }
        ],
        "errorLog": [
          {
            "timestamp": "2026-02-22T15:00:00Z",
            "task": "Schema migration v7",
            "errorType": "INCOMPLETE_OUTPUT",
            "severity": "medio",
            "attribution": "EXECUTION_FAIL",
            "orchImpact": -1,
            "note": "OlvidÃ³ campo maturity en migraciÃ³n"
          }
        ]
      },
      "aice-backend-dev": {
        "role": "backend",
        "model": "anthropic/claude-sonnet-4-5",
        "thinking": "high",
        "project": "ai-confidence",
        "persistent": false,
        "stats": {
          "tasksAssigned": 5,
          "tasksCompleted": 4,
          "tasksWithErrors": 1,
          "errorsCapturated": 0,
          "reliability": 0.80,
          "lastActivity": "2026-02-23T11:00:00Z"
        },
        "recentEvals": [],
        "errorLog": []
      }
    },
    "teamMetrics": {
      "totalTasks": 45,
      "totalErrors": 8,
      "avgReliability": 0.82,
      "orchContribution": {
        "streakBonus": 3,
        "penalties": -4,
        "netImpact": -1
      },
      "lastUpdated": "2026-02-23T12:30:00Z"
    }
  }
}
```

### 6.2 Campos del Registro por Agente

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `role` | string | Rol del agente (architect, backend, frontend, dataeng, tester) |
| `model` | string | Modelo LLM que usa |
| `thinking` | string | Nivel de thinking mode |
| `project` | string | Proyecto al que pertenece |
| `persistent` | boolean | Â¿Sobrevive al proyecto? (arquitectos: sÃ­, equipos: no) |
| `stats.tasksAssigned` | int | Tareas asignadas total |
| `stats.tasksCompleted` | int | Tareas completadas exitosamente |
| `stats.tasksWithErrors` | int | Tareas con errores (capturados o no) |
| `stats.errorsCapturated` | int | Errores del sub-agente que ComPi capturÃ³ antes de entregar |
| `stats.reliability` | float | `(completed - withErrors) / assigned` |
| `recentEvals` | array | Ãšltimas N evaluaciones (rolling window, max 20) |
| `errorLog` | array | Log de errores con atribuciÃ³n (append, max 50) |

### 6.3 MÃ©tricas de Equipo Agregadas

| Campo | DescripciÃ³n |
|-------|-------------|
| `totalTasks` | Total de tareas delegadas a todo el equipo |
| `totalErrors` | Errores totales del equipo |
| `avgReliability` | Media de reliability de todos los agentes activos |
| `orchContribution.streakBonus` | Puntos sumados al ORCH de ComPi por Ã©xitos del equipo |
| `orchContribution.penalties` | Puntos restados del ORCH de ComPi por fallos del equipo |
| `orchContribution.netImpact` | Impacto neto del equipo en el ORCH de ComPi |

### 6.4 Nuevo Comando: `/aice team`

```
/aice team              â†’ Resumen del equipo (reliability por agente + mÃ©tricas agregadas)
/aice team [agent-id]   â†’ Detalle de un agente especÃ­fico (recentEvals + errorLog)
/aice team errors       â†’ Ãšltimos errores del equipo con atribuciÃ³n
/aice team impact       â†’ Impacto neto del equipo en el ORCH de ComPi
```

**Ejemplo de output `/aice team`:**

```
ðŸŽ¯ AICE Team Registry â€” ai-confidence
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Agente               â”‚ Modelo       â”‚ Tareas â”‚ Errors â”‚ Fiabilidad
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸ—ï¸ Arquitecto        â”‚ Opus 4.6     â”‚  12    â”‚   2    â”‚   83%
âš™ï¸ Backend           â”‚ Sonnet 4.5   â”‚   5    â”‚   1    â”‚   80%
ðŸŽ¨ Frontend          â”‚ Sonnet 4.5   â”‚   3    â”‚   0    â”‚  100%
ðŸ—„ï¸ Data Engineer     â”‚ Sonnet 4.5   â”‚   8    â”‚   2    â”‚   75%
ðŸ§ª Tester            â”‚ Sonnet 4.5   â”‚  17    â”‚   3    â”‚   82%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸ“Š Equipo: 45 tareas | 8 errores | 82% fiabilidad media
ðŸŽ¯ Impacto en ORCH de ComPi: +3 streak bonus, -4 penalties = neto -1
```

---

## 7. Impacto en Velocidad de MaduraciÃ³n

### 7.1 Modelo Actual (Solo ComPi)

```
Estimaciones actuales (de SKILL.md Â§6.6):
  ~10 evaluaciones/dÃ­a (ComPi interactuando con Sergio)
  
  GREEN â†’ JUNIOR (100 evals):  ~10 dÃ­as
  JUNIOR â†’ ESTABLISHED (500):   ~40 dÃ­as
  ESTABLISHED â†’ MATURE (2000):  ~150 dÃ­as
  
  Total a MATURE: ~200 dÃ­as
```

### 7.2 Con Modelo D (Cascada Simplificada)

**ComPi (score pÃºblico):**
```
Evaluaciones directas de Sergio: ~10/dÃ­a (sin cambio)
Evaluaciones implÃ­citas de ORCH por equipo: ~5-15/dÃ­a (nuevas)
  - Cada tarea delegada que se completa = eval implÃ­cita de ORCH
  - Con 5 sub-agentes activos, ~3-5 tareas/dÃ­a cada uno

ORCH evaluaciones: +10/dÃ­a (conservador)
Otros dominios: sin cambio (los evalÃºa Sergio directamente)

Total evaluaciones/dÃ­a: ~20 (vs 10 actual)
AceleraciÃ³n: ~2x para el dominio ORCH

GREEN â†’ JUNIOR:  ~5 dÃ­as (vs 10)
JUNIOR â†’ ESTABLISHED: ~25 dÃ­as (vs 40) 
Total impacto: moderado, porque solo ORCH se acelera
```

**Sub-agentes (registro interno, NO maturity formal):**
```
Evaluaciones por sub-agente: ~3-5 tareas/dÃ­a
  â†’ 3-5 evals/dÃ­a por sub-agente
  â†’ 5 sub-agentes Ã— 5 evals = ~25 evals/dÃ­a para el equipo

Â¿Sirven para maturity de ComPi? 
  â†’ Solo las que impactan ORCH (todas)
  â†’ Las de otros dominios NO (las evalÃºa Sergio)
```

### 7.3 Con Modelo B Futuro (Scores Independientes)

```
Si cada sub-agente tiene score propio:
  ComPi evalÃºa a Arquitecto: ~5 evals/dÃ­a
  Arquitecto evalÃºa a equipo (4 agentes): ~3-5 evals/dÃ­a/agente
  EvaluaciÃ³n cruzada (Tester evalÃºa output de Backend): ~5-10/dÃ­a

Total sistema: ~50-80 evals/dÃ­a vs 10 actual
AceleraciÃ³n: ~5-8x

PERO: estas evaluaciones son inter-agente (no humanas)
  â†’ Peso reducido por defecto (Ã—0.5 como evaluaciÃ³n implÃ­cita)
  â†’ Efectivo: ~25-40 evals ponderadas/dÃ­a
  â†’ AceleraciÃ³n real: ~2.5-4x

Un sub-agente individual:
  5 evals/dÃ­a â†’ GREENâ†’JUNIOR en ~20 dÃ­as
  â†’ JUNIORâ†’ESTABLISHED en ~80 dÃ­as
  
MÃ¡s lento que ComPi (que tiene evaluaciÃ³n humana directa)
pero mÃ¡s rÃ¡pido de lo que serÃ­a sin inter-evaluaciÃ³n.
```

### 7.4 Riesgo: EvaluaciÃ³n Inter-Agente Inflada

```
âš ï¸ Problema potencial:
  Si agentes se evalÃºan entre sÃ­ sin supervisiÃ³n humana,
  existe riesgo de "echo chamber" donde todos se dan buenas notas.

MitigaciÃ³n (Modelo D):
  1. Inter-evaluaciÃ³n tiene peso 0.5 (vs 1.0 de evaluaciÃ³n humana)
  2. El registro de equipo es factual (tareas/errores/Ã©xitos), no scoring AICE
  3. La evaluaciÃ³n que realmente importa (ComPi por Sergio) sigue siendo humana
  4. Anti-gaming: el servidor central (Fase 2) detecta patrones de inflaciÃ³n
```

---

## 8. Consideraciones Adicionales

### 8.1 Anti-Gaming

| Riesgo | MitigaciÃ³n en Modelo D |
|--------|----------------------|
| ComPi atribuye culpa al sub-agente para proteger su ORCH | Reglas determinÃ­sticas: la atribuciÃ³n no es discrecional |
| Sub-agente infla su propio rendimiento | Sub-agentes no evalÃºan â€” ComPi registra factuales (tarea completada vs error) |
| ComPi crea "sub-agentes fantasma" para inflar ORCH | El registro incluye timestamp + tarea + output verificable |
| EvaluaciÃ³n benÃ©vola entre agentes | Fase 1: no hay inter-evaluaciÃ³n scoring. Solo registro factual |

### 8.2 Sub-Agentes EfÃ­meros vs Persistentes

| Tipo | Ejemplo | Tratamiento |
|------|---------|-------------|
| **EfÃ­mero** | Backend dev para proyecto X | Registro en teamRegistry. Cuando proyecto termina, se archiva con `active: false` |
| **Persistente** | Arquitecto cross-proyecto | Registro acumulativo. En Fase 2: score AICE propio |
| **One-shot** | Sub-agente para una tarea puntual | No se registra en teamRegistry (overhead > valor). Solo afecta ORCH de ComPi |

**Umbral para registro:** Solo se registran sub-agentes que tienen â‰¥3 tareas asignadas. Tasks one-shot afectan ORCH de ComPi directamente sin registro individual.

### 8.3 Transparencia para Sergio

**Nivel 1 (default):** Sergio ve el score de ComPi. ORCH refleja calidad de orquestaciÃ³n.

**Nivel 2 (bajo demanda):** Sergio pide "Â¿cÃ³mo va el equipo?" â†’ ComPi presenta teamRegistry formateado.

**Nivel 3 (alertas proactivas):** Si un sub-agente tiene reliability < 60%, ComPi alerta:
```
âš ï¸ El Backend Developer (Sonnet 4.5) tiene fiabilidad del 55% 
en las Ãºltimas 10 tareas. Â¿Cambio de modelo o ajuste de supervisiÃ³n?
```

### 8.4 Impacto en Skill AICE (Cambios Requeridos)

| Componente | Cambio | Esfuerzo |
|------------|--------|----------|
| `confidence.json` | AÃ±adir `teamRegistry` | Bajo |
| SKILL.md | Documentar teamRegistry + reglas de atribuciÃ³n | Medio |
| Comandos | AÃ±adir `/aice team` | Bajo |
| ORCH domain | Definir cÃ³mo teamRegistry afecta ORCH scoring | Medio |
| `confidence-log.jsonl` | AÃ±adir campo `attribution` a entries de ORCH | Bajo |
| Servidor central (Fase 2) | Model profiles, normalizaciÃ³n | Alto (futuro) |

---

## 9. DecisiÃ³n

### RecomendaciÃ³n: Modelo D â€” Cascada Simplificada con Registro de Equipo

**Fase 1 (Ahora):**
- AÃ±adir `teamRegistry` a `confidence.json` (versiÃ³n 8)
- Implementar reglas de atribuciÃ³n determinÃ­sticas (Â§5.1)
- Implementar `/aice team` para transparencia
- ORCH de ComPi refleja calidad de orquestaciÃ³n con datos del equipo
- Sub-agentes NO tienen score AICE propio

**Fase 2 (Con API central):**
- Agentes persistentes (Arquitectos) obtienen score AICE propio
- NormalizaciÃ³n por modelo (Model Performance Profiles)
- Anti-gaming del servidor central
- Inter-evaluaciÃ³n con peso 0.5

**Fase 3 (Multi-agente maduro):**
- Scores independientes para todos (Modelo B completo)
- Leaderboard multi-agente con normalizaciÃ³n
- EvaluaciÃ³n cruzada validada
- MÃ©tricas de equipo en dashboard

### Consecuencias

**Positivas:**
- Sergio puede evaluar al equipo, no solo a ComPi
- ComPi tiene datos para mejorar su orquestaciÃ³n
- Datos acumulados desde MVP, listos para Fase 2
- Sin complejidad innecesaria prematura

**Negativas:**
- Fase 1 no tiene scores de sub-agentes comparables entre proyectos
- La normalizaciÃ³n por modelo requiere datos del servidor central
- El registro de equipo es responsabilidad de ComPi (overhead de mantenimiento)

**Riesgos:**
- Si ComPi no mantiene el registro actualizdo â†’ datos desactualizados
  - MitigaciÃ³n: hook en delegaciÃ³n (similar a delegation-guard de EP-001)
- Si los datos factuales son incorrectos â†’ atribuciÃ³n incorrecta
  - MitigaciÃ³n: Sergio puede auditar el registro bajo demanda

---

*Siguiente paso: ValidaciÃ³n del PO (Sergio) y, si aprobado, implementaciÃ³n de teamRegistry en confidence.json v8.*
